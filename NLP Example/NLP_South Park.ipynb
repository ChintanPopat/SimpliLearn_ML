{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: using random word inputs, predict which South Park character is speaking from a list of top characters\n",
    "\n",
    "### Data source: https://www.kaggle.com/tovarischsukhov/southparklines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armando_Galeana\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Armando_Galeana\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "      <td>70896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3950</td>\n",
       "      <td>64301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>What?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6416</td>\n",
       "      <td>5271</td>\n",
       "      <td>9774</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Season Episode Character     Line\n",
       "count   70896   70896     70896    70896\n",
       "unique     19      19      3950    64301\n",
       "top         2      10   Cartman  What?\\n\n",
       "freq     6416    5271      9774      361"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "South_Park_raw = pd.read_csv('All-seasons.csv')\n",
    "South_Park_raw.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Season Episode Character                                               Line\n",
      "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
      "1     10       1      Kyle                        Going away? For how long?\\n\n",
      "2     10       1      Stan                                         Forever.\\n\n",
      "3     10       1      Chef                                  I'm sorry boys.\\n\n",
      "4     10       1      Stan  Chef said he's been bored, so he joining a gro...\n",
      "(70896, 4)\n"
     ]
    }
   ],
   "source": [
    "# Head and shape of dataset\n",
    "print(South_Park_raw.head())\n",
    "print(South_Park_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Season Episode Character     Line\n",
      "count   70896   70896     70896    70896\n",
      "unique     19      19      3950    64301\n",
      "top         2      10   Cartman  What?\\n\n",
      "freq     6416    5271      9774      361\n"
     ]
    }
   ],
   "source": [
    "print (South_Park_raw.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character\n",
      "Cartman         9774\n",
      "Stan            7680\n",
      "Kyle            7099\n",
      "Butters         2602\n",
      "Randy           2467\n",
      "Mr. Garrison    1002\n",
      "Chef             917\n",
      "Kenny            881\n",
      "Sharon           862\n",
      "Mr. Mackey       633\n",
      "Gerald           626\n",
      "Jimmy            597\n",
      "Wendy            585\n",
      "Liane            582\n",
      "Sheila           566\n",
      "Jimbo            556\n",
      "dtype: int64\n",
      "       Character     Line\n",
      "count      37429    37429\n",
      "unique        16    34196\n",
      "top      Cartman  What?\\n\n",
      "freq        9774      237\n"
     ]
    }
   ],
   "source": [
    "#Select just speakers with more than 500 lines\n",
    "\n",
    "top_speakers = South_Park_raw.groupby(['Character']).size().loc[South_Park_raw.groupby(['Character']).size() > 500]\n",
    "print (top_speakers.sort_values(ascending=False))\n",
    "\n",
    "#Select rows top speakers   \n",
    "\"\"\" This is the dataset we will be working with\"\"\"\n",
    "\n",
    "main_char_lines = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\n",
    "del main_char_lines['Season']\n",
    "del main_char_lines['Episode']\n",
    "\n",
    "main_char_lines = main_char_lines.reset_index(drop=True)\n",
    "\n",
    "print (main_char_lines.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = main_char_lines.Line\n",
    "y = main_char_lines.Character\n",
    "\n",
    "#print (y.value_counts(normalize=True))\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for best parameters to use in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "#pipe.steps\n",
    "\n",
    "#param_grid = {}\n",
    "#param_grid[\"tfidfvectorizer__max_features\"] = [500, 1000, 15000]\n",
    "#param_grid[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "#param_grid[\"tfidfvectorizer__lowercase\"] = [True, False]\n",
    "#param_grid[\"tfidfvectorizer__stop_words\"] = [\"english\", None]\n",
    "#param_grid[\"tfidfvectorizer__strip_accents\"] = [\"ascii\", \"unicode\", None]\n",
    "#param_grid[\"tfidfvectorizer__analyzer\"] = [\"word\", \"char\"]\n",
    "#param_grid[\"tfidfvectorizer__binary\"] = [True, False]\n",
    "#param_grid[\"tfidfvectorizer__norm\"] = [\"l1\", \"l2\", None]\n",
    "#param_grid[\"tfidfvectorizer__use_idf\"] = [True, False]\n",
    "#param_grid[\"tfidfvectorizer__smooth_idf\"] = [True, False]\n",
    "#param_grid[\"tfidfvectorizer__sublinear_tf\"] = [True, False]\n",
    "\n",
    "#grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "#Helpful for understanding how to create your param grid.\n",
    "#grid.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (This can take a while to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(grid.best_params_)\n",
    "#print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy within dataset:  0.405300702664\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(analyzer='word', stop_words='english', max_features = 850, ngram_range=(1, 1), \n",
    "                       binary=False, lowercase=True, norm=None, smooth_idf=True, strip_accents=None,\n",
    "                       sublinear_tf=True, use_idf=False)\n",
    "\n",
    "mcl_transformed = vect.fit_transform(X)\n",
    "\n",
    "nb_SP_Model = MultinomialNB()\n",
    "nb_SP_Model.fit(mcl_transformed, y)\n",
    "print (\"Model accuracy within dataset: \", nb_SP_Model.score(mcl_transformed, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with cross validation: 0.3333783092\n"
     ]
    }
   ],
   "source": [
    "print (\"Model accuracy with cross validation:\", cross_val_score(MultinomialNB(), mcl_transformed.toarray(), \n",
    "                                                                y, cv=5, scoring=\"accuracy\").mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cartman']  most likely said it.\n"
     ]
    }
   ],
   "source": [
    "# Predict on new text\n",
    "new_text = [\"Well, I guess we'll have to roshambo for it. I'll kick you in the nuts as hard as I can, then you kick me square in the nuts as hard as you can...\"]\n",
    "new_text_transform = vect.transform(new_text)\n",
    "\n",
    "print (nb_SP_Model.predict(new_text_transform),\" most likely said it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table with Characters' Line likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Character  Likelihood\n",
      "1        Cartman    0.916238\n",
      "5          Jimmy    0.054017\n",
      "4          Jimbo    0.013115\n",
      "7           Kyle    0.006765\n",
      "9   Mr. Garrison    0.003730\n",
      "0        Butters    0.001493\n",
      "6          Kenny    0.001316\n",
      "14          Stan    0.001206\n",
      "15         Wendy    0.000955\n",
      "11         Randy    0.000384\n",
      "8          Liane    0.000319\n",
      "10    Mr. Mackey    0.000175\n",
      "12        Sharon    0.000138\n",
      "3         Gerald    0.000093\n",
      "2           Chef    0.000030\n",
      "13        Sheila    0.000026\n"
     ]
    }
   ],
   "source": [
    "SP_prob=pd.DataFrame(nb_SP_Model.predict_proba(new_text_transform))\n",
    "SP_prob=pd.DataFrame.transpose(SP_prob)\n",
    "SP_prob.columns = ['Likelihood']\n",
    "\n",
    "top_speakers_index = top_speakers.reset_index()\n",
    "top_speakers_index.columns = ['Character', 'Lines']\n",
    "top_speakers_index = top_speakers_index.drop('Lines', 1)\n",
    "\n",
    "Result = pd.concat([top_speakers_index, SP_prob], axis=1)\n",
    "\n",
    "print (Result.sort_values('Likelihood',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate \"spamminess\" for the top 3 characters: Cartman, Stan and Kyle\n",
    "### Used to test common words pertaining to these characters more than to others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate \"spaminess\" for Cartman with detailed coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartman = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\n",
    "del cartman['Season']\n",
    "del cartman['Episode']\n",
    "\n",
    "cartman.Character[cartman.Character != 'Cartman'] = 'Not Cartman'\n",
    "cartman.Character[cartman.Character == 'Cartman'] = 'Cartman'\n",
    "print (cartman)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cartman.Character.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_cartman = cartman.Line\n",
    "y_cartman = cartman.Character\n",
    "vect_cartman =CountVectorizer(stop_words='english')\n",
    "Xdtm_cartman = vect_cartman.fit_transform(X_cartman)\n",
    "nb_cartman = MultinomialNB()\n",
    "nb_cartman.fit(Xdtm_cartman,y_cartman)\n",
    "nb_cartman.score(Xdtm_cartman,y_cartman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_cartman = vect_cartman.get_feature_names()\n",
    "len(tokens_cartman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (vect_cartman.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_cartman.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_cartman.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_count_cartman= nb_cartman.feature_count_[0,:]\n",
    "token_count_cartman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_count_not_cartman = nb_cartman.feature_count_[1, :]\n",
    "token_count_not_cartman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a DataFrame of tokens with their separate Not-Cartman and Cartman counts\n",
    "cartman_tokens = pd.DataFrame({'token':tokens_cartman, 'Cartman':token_count_cartman, 'Not_Cartman':token_count_not_cartman}).set_index('token')\n",
    "cartman_tokens.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add 1 to Cartman and Not Cartman counts to avoid dividing by 0\n",
    "cartman_tokens['Cartman'] = cartman_tokens.Cartman + 1\n",
    "cartman_tokens['Not_Cartman'] = cartman_tokens.Not_Cartman + 1\n",
    "cartman_tokens.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes counts the number of observations in each class\n",
    "nb_cartman.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the Cartman and Not Cartman counts into frequencies\n",
    "cartman_tokens['Cartman'] = cartman_tokens.Cartman / nb_cartman.class_count_[0]\n",
    "cartman_tokens['Not_Cartman'] = cartman_tokens.Not_Cartman / nb_cartman.class_count_[1]\n",
    "cartman_tokens.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the ratio of Cartman-to-Not_Cartman for each token\n",
    "cartman_tokens['spam_ratio'] = cartman_tokens.Cartman / cartman_tokens.Not_Cartman\n",
    "cartman_tokens.sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "cartman_tokens.sort_values('spam_ratio', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try looking up scores of different words\n",
    "word = \"nyah\"\n",
    "cartman_tokens.loc[word, 'spam_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Spamminess\" for Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stan = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\n",
    "del stan['Season']\n",
    "del stan['Episode']\n",
    "\n",
    "stan.Character[stan.Character != 'Stan'] = 'Not Stan'\n",
    "stan.Character[stan.Character == 'Stan'] = 'Stan'\n",
    "\n",
    "X_stan = stan.Line\n",
    "y_stan = stan.Character\n",
    "vect_stan =CountVectorizer(stop_words='english')\n",
    "Xdtm_stan = vect_stan.fit_transform(X_stan)\n",
    "nb_stan = MultinomialNB()\n",
    "nb_stan.fit(Xdtm_stan,y_stan)\n",
    "nb_stan.score(Xdtm_stan,y_stan)\n",
    "\n",
    "tokens_stan = vect_stan.get_feature_names()\n",
    "\n",
    "token_count_stan= nb_stan.feature_count_[0,:]\n",
    "token_count_not_stan = nb_stan.feature_count_[1, :]\n",
    "\n",
    "stan_tokens = pd.DataFrame({'token':tokens_stan, 'Stan':token_count_stan, 'Not_Stan':token_count_not_stan}).set_index('token')\n",
    "\n",
    "stan_tokens['Stan'] = stan_tokens.Stan + 1\n",
    "stan_tokens['Not_Stan'] = stan_tokens.Not_Stan + 1\n",
    "\n",
    "stan_tokens['Stan'] = stan_tokens.Stan / nb_stan.class_count_[0]\n",
    "stan_tokens['Not_Stan'] = stan_tokens.Not_Stan / nb_stan.class_count_[1]\n",
    "\n",
    "stan_tokens['spam_ratio'] = stan_tokens.Stan / stan_tokens.Not_Stan\n",
    "\n",
    "# examine the DataFrame sorted by spam_ratio\n",
    "stan_tokens.sort_values('spam_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Spamminess\" for Kyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kyle = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\n",
    "del kyle['Season']\n",
    "del kyle['Episode']\n",
    "\n",
    "kyle.Character[kyle.Character != 'Kyle'] = 'Not Kyle'\n",
    "kyle.Character[kyle.Character == 'Kyle'] = 'Kyle'\n",
    "\n",
    "X_kyle = kyle.Line\n",
    "y_kyle = kyle.Character\n",
    "\n",
    "vect_kyle = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 1), \n",
    "                       binary=False, lowercase=True, norm=None, smooth_idf=True, strip_accents=None,\n",
    "                       sublinear_tf=True, use_idf=False)\n",
    "\n",
    "#vect_kyle =CountVectorizer(stop_words='english')\n",
    "Xdtm_kyle = vect_kyle.fit_transform(X_kyle)\n",
    "nb_kyle = MultinomialNB()\n",
    "nb_kyle.fit(Xdtm_kyle,y_kyle)\n",
    "nb_kyle.score(Xdtm_kyle,y_kyle)\n",
    "\n",
    "tokens_kyle = vect_kyle.get_feature_names()\n",
    "\n",
    "token_count_kyle= nb_kyle.feature_count_[0,:]\n",
    "token_count_not_kyle = nb_kyle.feature_count_[1, :]\n",
    "\n",
    "kyle_tokens = pd.DataFrame({'token':tokens_kyle, 'Kyle':token_count_kyle, 'Not_Kyle':token_count_not_kyle}).set_index('token')\n",
    "\n",
    "kyle_tokens['Kyle'] = kyle_tokens.Kyle + 1\n",
    "kyle_tokens['Not_Kyle'] = kyle_tokens.Not_Kyle + 1\n",
    "\n",
    "kyle_tokens['Kyle'] = kyle_tokens.Kyle / nb_kyle.class_count_[0]\n",
    "kyle_tokens['Not_Kyle'] = kyle_tokens.Not_Kyle / nb_kyle.class_count_[1]\n",
    "\n",
    "kyle_tokens['spam_ratio'] = kyle_tokens.Kyle / kyle_tokens.Not_Kyle\n",
    "\n",
    "# examine the DataFrame sorted by spam_ratio\n",
    "kyle_tokens.sort_values('spam_ratio', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = vect.get_feature_names()\n",
    "\n",
    "token_count= nb_SP_Model.feature_count_[0,:]\n",
    "\n",
    "All_tokens = pd.DataFrame({'Token':tokens, 'Token_Count':token_count}).set_index('Token')\n",
    "All_tokens.sort(columns='Token_Count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
